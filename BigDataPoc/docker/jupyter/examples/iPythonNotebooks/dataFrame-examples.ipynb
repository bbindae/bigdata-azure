{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Row\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"dataFrameExamples\")\n",
    "sc = SparkContext(conf = conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [('Ankit', 25), ('Jalfaizy', 22), ('saurabh', 20), ('Bala', 26)]\n",
    "rdd = sc.parallelize(l)\n",
    "people = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "schemaPeople = sqlContext.createDataFrame(people)\n",
    "\n",
    "type(schemaPeople)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "| 44|8602|37.19|\n",
      "+---+----+-----+\n",
      "| 35|5368|65.89|\n",
      "|  2|3391|40.64|\n",
      "| 47|6694|14.98|\n",
      "| 29| 680|13.08|\n",
      "| 91|8900|24.59|\n",
      "| 70|3959|68.68|\n",
      "| 85|1733|28.53|\n",
      "| 53|9900|83.55|\n",
      "| 14|1505| 4.32|\n",
      "| 51|3378| 19.8|\n",
      "| 42|6926|57.77|\n",
      "|  2|4424|55.77|\n",
      "| 79|9291|33.17|\n",
      "| 50|3901|23.57|\n",
      "| 20|6633| 6.49|\n",
      "| 15|6148|65.53|\n",
      "| 44|8331|99.19|\n",
      "|  5|3505|64.18|\n",
      "| 48|5539|32.42|\n",
      "| 47|9900|25.66|\n",
      "+---+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders = sqlContext.read.csv(\"/home/jovyan/examples/data-samples/spark-examples/customer-orders.csv\", inferSchema = True, header = True)\n",
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 44: integer (nullable = true)\n",
      " |-- 8602: integer (nullable = true)\n",
      " |-- 37.19: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(department=Row(id='73802', name='DEV'), employees=[Row(firstName='Hyuk', lastName='Kim', email='hkim@lp.com', salary=90000), Row(firstName='Kevin', lastName='Stern', email='kStern@lp.com', salary=400000)])\n"
     ]
    }
   ],
   "source": [
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "\n",
    "employee1 = Employee('Basher','armbrust', 'bash@lp.com', 100000)\n",
    "employee2 = Employee('Hyuk', 'Kim', 'hkim@lp.com', 90000)\n",
    "employee3 = Employee('Kevin','Stern', 'kStern@lp.com', 400000)\n",
    "\n",
    "#print(Employee[0])\n",
    "#print(employee1)\n",
    "\n",
    "department1 = Row(id='12345', name='HR')\n",
    "department2 = Row(id='73802', name='DEV')\n",
    "department3 = Row(id='42727', name='Ops')\n",
    "\n",
    "departmentWithEmployee1 = Row(department=department1, employees=[employee1, employee2, employee3])\n",
    "departmentWithEmployee2 = Row(department=department2, employees=[employee2, employee3])\n",
    "\n",
    "print(departmentWithEmployee2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[department: struct<id:string,name:string>, employees: array<struct<firstName:string,lastName:string,email:string,salary:bigint>>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|  department|           employees|\n",
      "+------------+--------------------+\n",
      "| [12345, HR]|[[Basher, armbrus...|\n",
      "|[73802, DEV]|[[Hyuk, Kim, hkim...|\n",
      "+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departmentWithEmployees_Seq = [departmentWithEmployee1, departmentWithEmployee2]\n",
    "dframe = sqlContext.createDataFrame(departmentWithEmployees_Seq)\n",
    "display(dframe)\n",
    "dframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a5f654cf6f26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0morderRecord1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'3222'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L32728'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morderRecord1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mPy4JJavaError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \"\"\"\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    346\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[1;32m    347\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    346\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[1;32m    347\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------+-------+-----+----------+\n",
      "|  OrderDate| Region|    Rep|   Item|Units|Unit Price|\n",
      "+-----------+-------+-------+-------+-----+----------+\n",
      "| 4-Jul-2014|   East|Richard|Pen Set|   62|      4.99|\n",
      "|12-Jul-2014|   East|   Nick| Binder|   29|      1.99|\n",
      "|21-Jul-2014|Central| Morgan|Pen Set|   55|     12.49|\n",
      "|29-Jul-2014|   East|  Susan| Binder|   81|     19.99|\n",
      "| 7-Aug-2014|Central|Matthew|Pen Set|   42|     23.95|\n",
      "|15-Aug-2014|   East|Richard| Pencil|   35|      4.99|\n",
      "|24-Aug-2014|   West|  James|   Desk|    3|     275.0|\n",
      "| 1-Sep-2014|Central|  Smith|   Desk|    2|     125.0|\n",
      "|10-Sep-2014|Central|   Bill| Pencil|    7|      1.29|\n",
      "|18-Sep-2014|   East|Richard|Pen Set|   16|     15.99|\n",
      "|27-Sep-2014|   West|  James|    Pen|   76|      1.99|\n",
      "| 5-Oct-2014|Central| Morgan| Binder|   28|      8.99|\n",
      "|14-Oct-2014|   West| Thomas| Binder|   57|     19.99|\n",
      "|22-Oct-2014|   East|Richard|    Pen|   64|      8.99|\n",
      "|31-Oct-2014|Central| Rachel| Pencil|   14|      1.29|\n",
      "| 8-Nov-2014|   East|  Susan|    Pen|   15|     19.99|\n",
      "|17-Nov-2014|Central|   Alex| Binder|   11|      4.99|\n",
      "|25-Nov-2014|Central|Matthew|Pen Set|   96|      4.99|\n",
      "| 4-Dec-2014|Central|   Alex| Binder|   94|     19.99|\n",
      "|12-Dec-2014|Central|  Smith| Pencil|   67|      1.29|\n",
      "+-----------+-------+-------+-------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- OrderDate: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Rep: string (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      " |-- Units: integer (nullable = true)\n",
      " |-- Unit Price: double (nullable = true)\n",
      "\n",
      "['OrderDate', 'Region', 'Rep', 'Item', 'Units', 'Unit Price']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "officeSupply_df = sqlContext.read.csv(\"/home/jovyan/examples/data-samples/spark-examples/P1-OfficeSupplies.csv\", inferSchema = True, header=True)\n",
    "officeSupply_df.show()\n",
    "officeSupply_df.printSchema()\n",
    "print(officeSupply_df.columns)\n",
    "officeSupply_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-----+----------+-------------------+----+-----+\n",
      "| Region|    Rep|   Item|Units|Unit Price|        OrderPlaced|year|month|\n",
      "+-------+-------+-------+-----+----------+-------------------+----+-----+\n",
      "|   East|Richard|Pen Set|   62|      4.99|2014-07-04 00:00:00|2014|    7|\n",
      "|   East|   Nick| Binder|   29|      1.99|2014-07-12 00:00:00|2014|    7|\n",
      "|Central| Morgan|Pen Set|   55|     12.49|2014-07-21 00:00:00|2014|    7|\n",
      "|   East|  Susan| Binder|   81|     19.99|2014-07-29 00:00:00|2014|    7|\n",
      "|Central|Matthew|Pen Set|   42|     23.95|2014-08-07 00:00:00|2014|    8|\n",
      "|   East|Richard| Pencil|   35|      4.99|2014-08-15 00:00:00|2014|    8|\n",
      "|   West|  James|   Desk|    3|     275.0|2014-08-24 00:00:00|2014|    8|\n",
      "|Central|  Smith|   Desk|    2|     125.0|2014-09-01 00:00:00|2014|    9|\n",
      "|Central|   Bill| Pencil|    7|      1.29|2014-09-10 00:00:00|2014|    9|\n",
      "|   East|Richard|Pen Set|   16|     15.99|2014-09-18 00:00:00|2014|    9|\n",
      "|   West|  James|    Pen|   76|      1.99|2014-09-27 00:00:00|2014|    9|\n",
      "|Central| Morgan| Binder|   28|      8.99|2014-10-05 00:00:00|2014|   10|\n",
      "|   West| Thomas| Binder|   57|     19.99|2014-10-14 00:00:00|2014|   10|\n",
      "|   East|Richard|    Pen|   64|      8.99|2014-10-22 00:00:00|2014|   10|\n",
      "|Central| Rachel| Pencil|   14|      1.29|2014-10-31 00:00:00|2014|   10|\n",
      "|   East|  Susan|    Pen|   15|     19.99|2014-11-08 00:00:00|2014|   11|\n",
      "|Central|   Alex| Binder|   11|      4.99|2014-11-17 00:00:00|2014|   11|\n",
      "|Central|Matthew|Pen Set|   96|      4.99|2014-11-25 00:00:00|2014|   11|\n",
      "|Central|   Alex| Binder|   94|     19.99|2014-12-04 00:00:00|2014|   12|\n",
      "|Central|  Smith| Pencil|   67|      1.29|2014-12-12 00:00:00|2014|   12|\n",
      "+-------+-------+-------+-----+----------+-------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+-------+-------+-----+----------+-------------------+----+-----+\n",
      "| Region|    Rep|   Item|Units|Unit Price|        OrderPlaced|year|month|\n",
      "+-------+-------+-------+-----+----------+-------------------+----+-----+\n",
      "|Central|  Smith|   Desk|    2|     125.0|2014-09-01 00:00:00|2014|    9|\n",
      "|Central|   Bill| Pencil|    7|      1.29|2014-09-10 00:00:00|2014|    9|\n",
      "|   East|Richard|Pen Set|   16|     15.99|2014-09-18 00:00:00|2014|    9|\n",
      "|   West|  James|    Pen|   76|      1.99|2014-09-27 00:00:00|2014|    9|\n",
      "+-------+-------+-------+-----+----------+-------------------+----+-----+\n",
      "\n",
      "+-------+-------+-------+-----+----------+-------------------+----+-----+\n",
      "| Region|    Rep|   Item|Units|Unit Price|        OrderPlaced|year|month|\n",
      "+-------+-------+-------+-----+----------+-------------------+----+-----+\n",
      "|   East|Richard|Pen Set|   62|      4.99|2014-07-04 00:00:00|2014|    7|\n",
      "|   East|Richard| Pencil|   35|      4.99|2014-08-15 00:00:00|2014|    8|\n",
      "|   East|Richard|Pen Set|   16|     15.99|2014-09-18 00:00:00|2014|    9|\n",
      "|   East|Richard|    Pen|   64|      8.99|2014-10-22 00:00:00|2014|   10|\n",
      "|Central|   Alex| Binder|   11|      4.99|2014-11-17 00:00:00|2014|   11|\n",
      "|Central|   Alex| Binder|   94|     19.99|2014-12-04 00:00:00|2014|   12|\n",
      "|   East|Richard| Pencil|   95|      1.99|2015-01-06 00:00:00|2015|    1|\n",
      "|Central|   Alex| Pencil|   36|      4.99|2015-02-09 00:00:00|2015|    2|\n",
      "|   East|Richard| Binder|    4|      4.99|2015-02-18 00:00:00|2015|    2|\n",
      "|Central|   Alex|Pen Set|   50|      4.99|2015-03-24 00:00:00|2015|    3|\n",
      "|   East|Richard| Binder|   60|      4.99|2015-04-01 00:00:00|2015|    4|\n",
      "|Central|   Alex| Pencil|   90|      4.99|2015-05-05 00:00:00|2015|    5|\n",
      "|   East|Richard| Binder|   60|      8.99|2015-06-08 00:00:00|2015|    6|\n",
      "+-------+-------+-------+-----+----------+-------------------+----+-----+\n",
      "\n",
      "+-------+-----+\n",
      "| Region|count|\n",
      "+-------+-----+\n",
      "|Central|   24|\n",
      "|   East|   13|\n",
      "|   West|    6|\n",
      "+-------+-----+\n",
      "\n",
      "+-------+----------+-----+-----+-----+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|   Item|Unit Price|    1|    2|    3|   4|   5|    6|    7|    8|    9|   10|   11|   12|\n",
      "+-------+----------+-----+-----+-----+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|   Desk|     275.0| null| null| null|null|null| null| null|275.0| null| null| null| null|\n",
      "| Pencil|      2.99| null| null| 2.99|null|null| null| null| null| null| null| null| null|\n",
      "| Pencil|      4.99| null| 4.99| null|null|4.99| 4.99| null| 4.99| null| null| null| null|\n",
      "|    Pen|      4.99| null| null| null|4.99|null| null| null| null| null| null| null| null|\n",
      "|    Pen|      8.99| null| null| null|null|null| null| null| null| null| 8.99| null| null|\n",
      "|    Pen|      1.99| null| null| null|null|null| null| null| null| 1.99| null| null| null|\n",
      "| Pencil|      1.29| null| null| null|null|1.29| null| null| null| 1.29| 1.29| null| 1.29|\n",
      "| Pencil|      1.99| 1.99| null| null|3.98|1.99| null| null| null| null| null| null| null|\n",
      "| Binder|      15.0| null| 15.0| null|null|null| null| null| null| null| null| null| null|\n",
      "|    Pen|     19.99| null|19.99| null|null|null| null| null| null| null| null|19.99| null|\n",
      "|Pen Set|     15.99| null| null| null|null|null| null| null| null|15.99| null| null|15.99|\n",
      "|   Desk|     125.0| null| null| null|null|null|125.0| null| null|125.0| null| null| null|\n",
      "| Binder|      4.99| null| 4.99| null|4.99|null| null| null| null| null| null| 4.99| 4.99|\n",
      "|Pen Set|      4.99| null| null| 4.99|null|null| null| 4.99| null| null| null| 4.99| null|\n",
      "|Pen Set|     12.49| null| null| null|null|null| null|12.49| null| null| null| null| null|\n",
      "| Binder|      1.99| null| null| null|null|null| null| 1.99| null| null| null| null| null|\n",
      "|Pen Set|     23.95| null| null| null|null|null| null| null|23.95| null| null| null| null|\n",
      "| Binder|      8.99| 8.99| null| null|null|8.99| 8.99| null| null| null| 8.99| null| null|\n",
      "| Binder|     19.99|19.99| null|19.99|null|null| null|19.99| null| null|19.99| null|19.99|\n",
      "+-------+----------+-----+-----+-----+----+----+-----+-----+-----+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, year, month\n",
    "\n",
    "# Convert string column, \"OrderDate\" into DateTime\n",
    "d = officeSupply_df.select(officeSupply_df.Region, officeSupply_df.Rep, \\\n",
    "                          officeSupply_df.Item, officeSupply_df.Units, officeSupply_df.columns[5], \\\n",
    "                          to_timestamp(officeSupply_df.OrderDate, 'dd-MMM-yyyy').alias('OrderPlaced'))\n",
    "\n",
    "#extract year and month from OrderPlaced\n",
    "d = d.select(d.Region, d.Rep, d.Item, d.Units, d.columns[4], d.OrderPlaced, year(d.OrderPlaced).alias('year'), \\\n",
    "             month(d.OrderPlaced).alias('month') \\\n",
    "            )\n",
    "             \n",
    "d.show()\n",
    "d.describe(\"Unit Price\")\n",
    "\n",
    "d.filter(d.month == 9).show()\n",
    "d.filter(d.month == 11).count()\n",
    "d.filter((d.Rep=='Richard') | (d.Rep=='Alex')).show()\n",
    "d.groupby(\"Region\").count().show()\n",
    "\n",
    "d.groupby(\"Item\", \"Unit Price\").pivot(\"month\").sum(\"Unit Price\").show()\n",
    "#d = officeSupply_df.select(to_timestamp(officeSupply_df.OrderDate, 'dd-MMM-yyyy').alias('dt'))\n",
    "#print(d)\n",
    "#d.printSchema()\n",
    "#Convert string to datetime and extract only month part\n",
    "#d = officeSupply_df.select(officeSupply_df.Region, officeSupply_df.Item, officeSupply_df.Units,month(to_timestamp(officeSupply_df.OrderDate, 'dd-MMM-yyyy')).alias('dt') )\n",
    "#d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
