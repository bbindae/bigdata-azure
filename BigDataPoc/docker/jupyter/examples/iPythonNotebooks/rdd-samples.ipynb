{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This file demonstrate RDD operations\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# instantiate spark context\n",
    "sc = SparkContext(\"local\", \"rdd-sample\")\n",
    "\n",
    "words = sc.parallelize([\"scala\", \"java\", \"C#\", \"vb.net\", \"F#\", \"objective-c\", \"python\", \"javascript\", \"react\", \"hadoop\", \"hive\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in RDD: 11\n"
     ]
    }
   ],
   "source": [
    "#count() Number of elements in the RDD in returned.\n",
    "\n",
    "counts = words.count()\n",
    "print(\"Number of elements in RDD: %i\" % (counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in RDD: ['scala', 'java', 'C#', 'vb.net', 'F#', 'objective-c', 'python', 'javascript', 'react', 'hadoop', 'hive']\n"
     ]
    }
   ],
   "source": [
    "#collect() All the elements in the RDD are returned.\n",
    "coll = words.collect()\n",
    "print(\"Elements in RDD: %s\" % coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#foreach(f) returns only those elements which meet the condition of the function inside foreach.\n",
    "\n",
    "#define function\n",
    "def f(x):\n",
    "    print(\"element: %s\" % x)\n",
    "\n",
    "fore = words.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitered RDD: ['scala', 'java', 'javascript', 'react', 'hadoop']\n"
     ]
    }
   ],
   "source": [
    "#filter(f) returns the elements which satisfies the function inside the filter\n",
    "\n",
    "words_filter = words.filter(lambda x: 'a' in x)\n",
    "print(\"Fitered RDD: %s\" % words_filter.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key value pair: [('scala', 1), ('java', 1), ('C#', 1), ('vb.net', 1), ('F#', 1), ('objective-c', 1), ('python', 1), ('javascript', 1), ('react', 1), ('hadoop', 1), ('hive', 1)]\n"
     ]
    }
   ],
   "source": [
    "#map(f) returns a new RDD by applying a function to each element in the RDD\n",
    "words_map = words.map(lambda x: (x, 1))\n",
    "mapping = words_map.collect()\n",
    "print(\"Key value pair: %s\" % mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding all the elements: 55\n"
     ]
    }
   ],
   "source": [
    "#reduce(f)\n",
    "# After performing the specified commutative and associative binary operation, the element in the RDD is returned.\n",
    "\n",
    "from operator import add\n",
    "numbers = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "adding = numbers.reduce(add)\n",
    "print(\"Adding all the elements: %i\" % adding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join RDD: [('hadoop', (4, 6)), ('spark', (1, 3))]\n"
     ]
    }
   ],
   "source": [
    "#join(other)\n",
    "# returns RDD with a pair of elements with the matching keys and all the values for that particular key.\n",
    "\n",
    "x = sc.parallelize([(\"spark\",1), (\"hadoop\",4)])\n",
    "y = sc.parallelize([(\"spark\", 3), (\"hadoop\", 6)])\n",
    "\n",
    "joined = x.join(y)\n",
    "print(\"Join RDD: %s\" % joined.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words got cached: True\n"
     ]
    }
   ],
   "source": [
    "#cache()\n",
    "# persis the RDD with the default storage level (MEMORY_ONLY)\n",
    "\n",
    "words = sc.parallelize([\"lampsplus\", \"amazon\", \"microsoft\", \"google\"])\n",
    "words.cache()\n",
    "print(\"Words got cached: %s\" % words.persist().is_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
